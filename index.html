<!DOCTYPE html>
<html>
<head>
    <title>Yilei Chen</title>

    <!-- Meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Styles -->
    <style>
          body {
            font-family: Roboto, 'sans-serif';
            font-size: 16px;
            background-color: #FFFFFF;
            color: #4F6071;
          }
          h1 {
              font-weight: 300;
              font-size: 2rem;
          }
          #header {
            background-color: #f4f4f4;
            /*background-color: #FFFFFF;*/
            display: flex;
            align-items: flex-end;
            padding-top:60px;
            padding-bottom:60px;
          }
          #footer {
            background-color: #FFFFFF;
            padding:60px;
          }
          #portrait {
            border: 3px solid white;
          }
          #header-text {
            margin-top: 60px;
            margin-left: 220px;
          }
          #header-text-name {
            font-size: 40px;
          }
          #header-text-title {
              font-size: 17px;
          }
          #header-text-affiliation {
              font-size: 17px;
              margin-top: 10px;
          }
          #header-text-email {
            margin-top: 10px;
            font-size: 17px;
            font-style: italic;
          }
          .header-text-desc {
            font-size: 20px;
          }
          .vspace-top {
            margin-top: 30px;
          }
          .vspace-top-news {
              margin-top: 15px;
          }
          .paper-image {
            width: 150px;
          }
          .news-date {
              font-weight: bold;
          }
          .paper-title {
            font-weight: bold;
          }
          .paper-authors {
            font-style: italic;
          }
	  .technical-abstract {
    font-size: 0.95rem;
}

.abstract-title {
    font-weight: 500;
    color: #2c3e50;
    letter-spacing: 0.5px;
}

.abstract-paragraph {
    margin-bottom: 1.2rem;
    line-height: 1.7;
    color: #4a5568;
}

.abstract-paragraph strong {
    color: #2c3e50;
    font-weight: 500;
}

/* 增强印刷质感 */
@media print {
    .technical-abstract {
        break-inside: avoid;
    }
    .resource-links {
        display: none;
    }
}
	  /* 添加语言切换按钮样式 */
          #language-switcher {
		  position: fixed;
		  top: 20px;
		  right: 20px;
		  z-index: 1000;
	  }
	  .lang-btn {
		  background: none;
		  border: 1px solid #4F6071;
		  padding: 5px 10px;
		  cursor: pointer;
		  margin-left: 5px;
	  }
	    .lang-btn.active {
		    background: #4F6071;
		    color: white;
	    }
    </style>
</head>

<body>
    <div id="language-switcher">
	    <button class="lang-btn active" id="zh-btn">中文</button>
	    <button class="lang-btn" id="en-btn">EN</button>
    </div>
	
    <div id='header'>
        <div class='container'>
            <div class='row'>
                <div class="col-sm-3 offset-sm-1">
                    <img src='imgs/portrait.jpg' class='img-fluid' id='portrait'>
                </div>

                <div class="col">
                  <div id='header-text-name' data-translate="header.name"></div>
                  <div id='header-text-title' data-translate="header.title"></div>
                  <div id='header-text-affiliation' data-translate="header.affiliation"></div>
                  <div id='header-text-email' data-translate="header.email"></div>
                  <div>
		    <a href="docs/cv_yileichen_zh.pdf">[CV]</a>
                    <a href="https://github.com/yileichen96">[GitHub]</a>
                    <a href="https://scholar.google.com/citations?user=HYPiQyUAAAAJ&hl=zh-CN">[Google Scholar]</a>
                    <!-- <a href="https://linkedin.com/in/vincentsitzmann">[LinkedIn]</a> -->  
                  </div>

                </div>
            </div>
        </div>
    </div>


    <div class='container'>
        <div class='row vspace-top'>
            <div class='col offset-sm-1'>
		<h1 class="fs-6">教育经历</h1>
		    <ul style="list-style-type: disc; padding-left: 1.2em;">
			<li>
			    <div style="display: flex; justify-content: space-between;">
	  		        <div>
		                <strong>悉尼科技大学 (UTS)</strong><br>
                                信息系统 博士，Faculty of Engineering and Information Technology<br>
                                上海大学-UTS 双学位博士项目，CSC 国家留学基金委资助，导师：A/Prof. <a href="https://profiles.uts.edu.au/qiang.wu">Qiang Wu</a>
                                </div>
                                <div style="text-align: right; white-space: nowrap;">
                                2024.02 – 2025.09<br>
                                悉尼
                                </div>
                           </div>
                        </li>

                        <li style="margin-top: 1em;">
                            <div style="display: flex; justify-content: space-between;">
                                <div>
                                <strong>上海大学</strong><br>
                                信号与信息处理 博士，通信与信息工程学院<br>
                                硕博连读，研究方向：光场/多视点图像处理，导师：<a href="https://scie-ie.shu.edu.cn/info/1078/1139.htm">安平</a> 教授
                                </div>
                                <div style="text-align: right; white-space: nowrap;">
                                2018.09 – 2025.06<br>
                                上海
                                </div>
                            </div>
                        </li>

                        <li style="margin-top: 1em;">
                            <div style="display: flex; justify-content: space-between;">
                                <div>
                                <strong>上海大学</strong><br>
                                电子信息工程 本科，通信与信息工程学院
                                </div>
                                <div style="text-align: right; white-space: nowrap;">
                                2014.09 – 2018.06<br>
                                上海
                                </div>
                           </div>
                        </li>
                </ul> 

		<div class="vspace-top d-flex align-items-baseline flex-wrap">
		    <h1 class="fs-6" data-translate="bio.projects_title">项目经历</h1>
		</div>
		    <ul style="list-style-type: disc; padding-left: 1.2em;">
		        <li>
                            <div style="display: flex; justify-content: space-between;">
				<div>
				<strong>动态光场的高效编码与高质量重建（国家自然科学基金-国际合作重点项目）</strong><br>
                                主要参与，负责探索高维光场的低秩稀疏理论、关键视点选择方法，并基于低秩稀疏理论开展基于显式深度与隐式深度结合的高质量稠密视点重建算法研究，以及基于重建算法的高效率光场图像压缩算法研究。
                                </div>
                                <div style="text-align: right; white-space: nowrap;">
                                2021.01 – 至今
                                </div>
                            </div>
                       </li>

                       <li style="margin-top: 1em;">
                            <div style="display: flex; justify-content: space-between;">
                                <div>
                                <strong>上海市3D内容制作专业技术服务平台（上海市科委研发平台建设专项）</strong><br>
                                主要参与，负责基于深度信息与纹理信息交互的光场图像显著性区域检测算法研究，为3D内容制作提供先验性指导，并参与实验室搭建多视光场采集与重建平台、虚拟增强现实平台。
                                </div>
                                <div style="text-align: right; white-space: nowrap;">
                                 2020.01 – 2023.12
                                </div>
                            </div>
                       </li>

                     <li style="margin-top: 1em;">
                          <div style="display: flex; justify-content: space-between;">
                              <div>
                              <strong>光场图像深度信息提取研究（国家自然科学基金-海外及港澳学者合作项目）</strong><br>
                              主要参与，负责基于光场深度信息提取的光场图像压缩、多光场间拼接算法研究。
                              </div>
                              <div style="text-align: right; white-space: nowrap;">
                              2019.01 – 2020.12
                              </div>
                         </div>
                    </li>
                </ul>

		<div class="vspace-top d-flex align-items-baseline flex-wrap">
		    <h1 class="fs-6">研究方向</h1>
		</div>
		    <ul style="list-style-type: disc; padding-left: 1.2em;">
			    <li>光场（Light Field）与多视点图像处理，包括光场数据的稀疏性探索、基于光场深度/视差信息的密集视点合成、压缩、显著性目标检测。
			    </li>
		    </ul>

                <div class="vspace-top d-flex align-items-baseline flex-wrap">
		    <h1 class="fs-6" data-translate="publications_first.title">发表论文</h1>
		    <span style="display: inline-block; width: 1em;"></span> <!-- 自定义2字符宽度空格 -->
		    <small class="text-body-secondary" data-translate="publications_first.note">  * 为共同第一作者</small>
		</div>

                <!-- List of publications -->

		<div class="mt-2">
		    <p> 1）结构化/非结构化光场重构（结构化光场重构：角度域超分辨；非结构化光场重构：任意视点合成）</p>
		</div>
		<div class='row'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/3DVET_video_leaves.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
			    Volume Feature Aware View-Epipolar Transformers for Generalizable NeRF
                        </div>
                        <div class='paper-desc'>
			    IEEE Transactions on Visualization and Computer Graphics 2025, under review
                        </div>
                        <div class='paper-authors'>
                            <b>Yilei Chen</b>, Ping An, Xinpeng Huang, Qiang Wu 
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph" data-translate="papers.vfavet.abstract1"></p>
                        </div>
                    </div>
                </div>
		    
		<div class='row vspace-top'>
                    <div class="col-sm-3">
			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/LFR-ELFR.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
			    Enhanced Light Field Reconstruction by Combining Disparity and Texture Information in PSVs via Disparity-Guided Fusion
			    <span style="font-weight: normal;"><a href="https://github.com/yileichen96/LFASR-ELFR">[Code]</a> <a href="https://ieeexplore.ieee.org/document/10158790">[Paper]</a></span> 
                        </div>
                        <div class='paper-desc'>
			    IEEE Transactions on Computational Imaging 2023, 9: 665-677
                        </div>
                        <div class='paper-authors'>
                            <b>Yilei Chen</b>, Xinpeng Huang, Ping An, Qiang Wu 
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            通过视差感知的掩膜机制，自适应结合显式深度重构流程（基于视差图估计与warping）与隐式深度重构流程（不依靠视差图，直接隐式建模视点间对应关系完成纹理映射）分别在大视差区域的抗混叠能力和小视差区域的细节恢复优势，在不同重构任务（2×2→7×7内插、外插，4→7×7灵活重构）实现更高的质量。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 并行的双分支框架，通过两类重构流程的稠密光场生成；(2) 基于平面扫描体（PSV）的视差/纹理信息高效解耦提取；(3) 基于可学习阈值的视差引导掩膜生成方法，充分发挥两类光场在不同视差区域的优势。
			    </p>
                       </div>
                    </div>
                </div>

		<div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/LFR-DELFASR.jpg' class='img-fluid'>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
                            Disparity Enhancement-based Light Field Angular Super-Resolution
			    <span style="font-weight: normal;"><a href="https://ieeexplore.ieee.org/abstract/document/10750480">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
                            IEEE Signal Processing Letters 2025, 32: 81-85
                        </div>
                        <div class='paper-authors'>
                            Dongjun Cai, <b>Yilei Chen</b>, Xinpeng Huang, Ping An
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            从视差图增强的角度提升重构视点质量，通过可学习的形态学滤波对场景物体边缘处估计的视差值进行自适应优化，缓解显式深度重构流程中由于遮挡导致的不正确采样内容，在大基线光场数据集达到先进性能。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 基于视差图估计与warping的显式深度光场重构算法；(2) 基于平面扫描体（PSV）的视差图估计；(3) 可学习的形态学滤波（膨胀和腐蚀），自适应修正物体边缘处的视差值。
			    </p>
                       </div>
                    </div>
                </div>

		<div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/LFR-GAMRNet.jpg' class='img-fluid'>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
                            Geometry-Assisted Multi-Representation View Reconstruction Network for Light Field Image Angular Super-Resolution
			    <span style="font-weight: normal;"><a href="https://github.com/ldyorchid/Geometry-Assisted-Multi-Representation-View-Reconstruction-Network-for-LFI-Angular-Super-resolution">[Code]</a> <a href="https://www.sciencedirect.com/science/article/pii/S0950705123001405">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
                            Knowledge-Based Systems 2023, 267: 110390
                        </div>
                        <div class='paper-authors'>
                            Deyang Liu, Zaidong Tong, Yan Huang, <b>Yilei Chen</b>, Yifan Zuo, Yuming Fang
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            充分利用光场多种表示形式（透镜图像、多视点图像阵列和伪视频序列）实现稠密光场重构，并通过光场间空间-角度信息和光场内几何信息进一步优化重构结果。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 光场多种表示形式下的稠密光场重构方法；(2) 几何辅助的优化模块。
			    </p>
                       </div>
                    </div>
                </div>

		<div class='vspace-top'>
		    <p> 2）光场压缩</p>
		</div>
		<div class='row'>
                    <div class="col-sm-3">
                        <img src='imgs/LFC-PoDR.jpg' class='img-fluid'>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
			    Prediction-Oriented Disparity Rectification Model for Geometry-Based Light Field Compression
			    <span style="font-weight: normal;"><a href="https://ieeexplore.ieee.org/document/10006377">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
			    IEEE Transactions on Broadcasting 2023, 69(1): 62-74
                        </div>
                        <div class='paper-authors'>
                            Xinpeng Huang*, <b>Yilei Chen</b>*, Ping An, Liquan Shen 
                        </div>
			<br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            以视点预测质量引导的视差图校正算法，优化视差图4D关联性以及恢复实际的非均匀视差分布特性，降低视差图码率消耗、增强基于视差图的视点预测能力，提升光场压缩性能。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 编码端传输稀疏视点+视差图、解码端基于视差图预测的光场压缩算法；(2) 多视点图像阵列结构先验引导的视差图4D关联性优化；(3) 根据视点预测质量迭代计算视差偏离量，恢复非均匀视差分布特性。
			    </p>
                       </div>
                    </div>
                </div>

		<div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/LFC-MPI.jpg' class='img-fluid'>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
			    Light Field Compression Using Global Multiplane Representation and Two-Step Prediction
			    <span style="font-weight: normal;"><a href="https://ieeexplore.ieee.org/document/9120185">[Paper]</a></span> 
                        </div>
                        <div class='paper-desc'>
			    IEEE Signal Processing Letters 2020, 27: 1135-1139
                        </div>
                        <div class='paper-authors'>
                            <b>Yilei Chen</b>, Ping An, Xinpeng Huang, Chao Yang, Deyang Liu, Qiang Wu 
                        </div>
			<br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            将稀疏视点生成的多平面图像（Multiplane Image，MPI）表示应用于光场压缩，提升解码端预测光场的角度-空间一致性，并在中低码率段具有优异率失真性能。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 编码端传输稀疏视点、解码端基于MPI预测的光场压缩算法；(2) 基于平面扫描体（PSV）的MPI生成；(3) 通过预测质量统计的关键稀疏视点选择。
			    </p>
                       </div>
                    </div>
                </div>

		<div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/LFC-GCC.jpg' class='img-fluid'>
                        </video>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
                            Low Bitrate Light Field Compression With Geometry and Content Consistency
			    <span style="font-weight: normal;"><a href="https://ieeexplore.ieee.org/document/9305734">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
                            IEEE Transactions on Multimedia 2022, 24: 152-165
                        </div>
                        <div class='paper-authors'>
                            Xinpeng Huang, Ping An, <b>Yilei Chen</b>, Deyang Liu, Liquan Shen
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            通过同时考虑稀疏视点间内容一致性与传输视差图几何一致性，提升解码端预测光场的角度-空间一致性，并增强光场压缩性能。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 编码端传输稀疏视点+视差图、解码端基于视差图预测的光场压缩算法；(2) 视点图像引导的视差图优化，提升几何一致性；(3) 基于内容相似性的稀疏视点排序与GOP编码结构设计，提升内容一致性。
			    </p>
                       </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/LFC-EMH.jpg' class='img-fluid'>
                        </video>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
                            Energy-Driven Reference Selection for Hierarchical Light Field Compression
			    <span style="font-weight: normal;"><a href="https://www.sciencedirect.com/science/article/pii/S0923596522001321">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
                            Signal Processing: Image Communication 2022, 109: 116849
                        </div>
                        <div class='paper-authors'>
                            Xinpeng Huang, Ping An, <b>Yilei Chen</b>, Deyang Liu
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            根据场景内容选择关键稀疏视点，并根据关键-非关键稀疏视点构建光场压缩分层预测结构。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 编码端传输稀疏视点+视差图、解码端基于视差图预测的光场压缩算法；(2) 低秩分析下能量保留程度驱动的关键稀疏视点选择；(3) 基于关键稀疏视点的分层预测结构。
			    </p>
                       </div>
                    </div>
	        </div>

		<div class='vspace-top'>
		    <p> 3）光场显著性目标检测</p>
		</div>
                <div class='row'>
                    <div class="col-sm-3">
                        <img src='imgs/LFSOD-CDINet.jpg' class='img-fluid'>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
			    Light Field Salient Object Detection With Sparse Views via Complementary and Discriminative Interaction Network
                            <span style="font-weight: normal;"><a href="https://github.com/yileichen96/LFSOD-CDINet">[Code]</a> <a href="https://ieeexplore.ieee.org/document/10168184">[Paper]</a></span> 
	                </div>
                        <div class='paper-desc'>
			    IEEE Transactions on Circuits and Systems for Video Technology 2024, 34(2): 1070-1085
                        </div>
                        <div class='paper-authors'>
                            <b>Yilei Chen</b>*, Gongyang Li*, Ping An, Zhi Liu, Xinpeng Huang, Qiang Wu 
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            通过定义结构化光场的关键稀疏视点表示，解决传统光场显著性目标检测方法依赖密集视点提供深度信息、在资源受限场景下实际部署存在困难的问题。在显著减少视点数量（49→5）的情况下，实现最先进的检测性能，并且在推理速度上达到当前光场显著性目标检测的最快水平，为光场数据的实际应用提供了高效解决方案。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 通过低秩约束定义结构化光场的关键稀疏视点表示；(2)设计RGB-光场双流“特征-显著图交互”机制，充分发挥关键稀疏视点的深度信息进行显著性目标检测。
			    </p>
                       </div>
                    </div>
                </div>

		<div class='vspace-top'>
		    <p> 4）光场前景去遮挡</p>
		</div>
                <div class='row'>
                    <div class="col-sm-3">
                        <img src='imgs/LFDeocc-MANet.jpg' class='img-fluid'>
                        </video>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
                            Mask-Aware Light Field De-Occlusion with Gated Feature Aggregation and Texture-Semantic Attention
			    <span style="font-weight: normal;"><a href="https://ieeexplore.ieee.org/document/10891423">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
                            IEEE Transactions on Multimedia 2025
                        </div>
                        <div class='paper-authors'>
                            Jieyu Chen, Ping An, Xinpeng Huang, <b>Yilei Chen</b>, Chao Yang, Liquan Shen
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            同样包括前景遮挡物定位（掩膜）与被遮挡内容恢复两个步骤，重点解决特征整合阶段对遮挡物和被遮挡区域的无区分处理，以及大面积遮挡区域下的纹理细节丢失。设计的网络同时具有轻量性。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 由掩膜引导的门卷积，更好区分来自遮挡物和被遮挡区域的特征；(2) 纹理-语义注意力机制，增强恢复的被遮挡内容细节；(3)基于无遮挡图像特征的知识蒸馏，促进特征提取过程。
			    </p>
                       </div>
                    </div>
                </div>
		    
                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/LFDeocc-LFORNet.jpg' class='img-fluid'>
                        </video>
                    </div>		
		<div class="col">
                        <div class='paper-title'>
                            Light Field Occlusion Removal Network via Foreground Location and Background Recovery
			    <span style="font-weight: normal;"><a href="https://www.sciencedirect.com/science/article/pii/S0923596522001345">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
                            Signal Processing: Image Communication 2022, 109: 116849
                        </div>
                        <div class='paper-authors'>
                            Shiao Zhang, <b>Yilei Chen</b>, Ping An, Xinpeng Huang, Chao Yang
                        </div>
		        <br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            在光场中，由于中心视点被遮挡的场景内容会被其他视点记录，可实现遮挡物去除。围绕这一特性设计网络，实现比单一2D图像更优的场景遮挡物去除效果。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 两个子网络分别利用光场的多视点信息定位前景遮挡物（基于掩膜）以及恢复被遮挡内容。
			    </p>
                       </div>
                    </div>
	        </div>

		<div class='vspace-top'>
		    <p> 5）多光场拼接</p>
		</div>
		<div class='row'>
                    <div class="col-sm-3">
                        <img src='imgs/LFS-MDALRF.jpg' class='img-fluid'>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
			    Light Field Stitching Via Mesh Deformation Alignment and Low-Rank-Based Fusion
			    <span style="font-weight: normal;"><a href="https://link.springer.com/chapter/10.1007/978-981-97-8692-3_35">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
			    Chinese Conference on Pattern Recognition and Computer Vision (PRCV) 2024: 497-508
                        </div>
                        <div class='paper-authors'>
                            <b>Yilei Chen</b>, Ping An, Chao Xue, Xinpeng Huang
                        </div>
			<br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            采用局部网格并施加基于视差图的几何约束，实现光场间更好的对齐，并根据光场低秩特性设计在基图像的2D图割，在极大减少计算量的同时，取得与4D图割相似的融合质量。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 由拼接后的中心视点图像根据视差图传播网格变形控制点；(2) 光场低秩特性引导下的2D图割，简化4D图割计算量。
			    </p>
                       </div>
                    </div>
                </div>

		<div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/LFS-MBLFS.jpg' class='img-fluid'>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
			    Modified Baseline for Light Field Stitching
			    <span style="font-weight: normal;"><a href="https://github.com/yileichen96/Modified-Baseline-for-Light-Field-Stitching">[Code]</a> <a href="https://ieeexplore.ieee.org/document/8965971">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
			    IEEE Visual Communications and Image Processing (VCIP) 2019: 1-4
                        </div>
                        <div class='paper-authors'>
                            <b>Yilei Chen</b>, Ping An, Xinpeng Huang, Chunli Meng, Qiang Wu 
                        </div>
			<br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            将2D图像拼接的基线方法（特征点提取与匹配→单应性矩阵估计与图像对齐→重叠区域的2D图割融合）扩展至4D光场，实现保留空间-角度一致性的光场拼接，扩大所有视点图像的视场。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 基于中心视点视差图的特征点匹配关系传播；(2) 基于直接线性变换（DLT）算法和Levenberg-Marquardt优化的4D单应性矩阵参数估计；(3) 相邻视点信息参与的改进2D图割（伪4D图割）。
			    </p>
                       </div>
                    </div>
                </div>

		<div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/ShuffleHomoNet.jpg' class='img-fluid'>
                    </div>
                    <div class="col">
                        <div class='paper-title'>
                            Fast and Accurate Homography Estimation Using Extendable Compression Network
			    <span style="font-weight: normal;"><a href="https://ieeexplore.ieee.org/document/9506264">[Paper]</a></span>
                        </div>
                        <div class='paper-desc'>
                            IEEE International Conference on Image Processing (ICIP) 2021: 1024-1028
                        </div>
                        <div class='paper-authors'>
                            <b>Yilei Chen</b>, Guoping Huang, Ping An, Zhixiang You, Xinpeng Huang
                        </div>
			<br> 
		        <!-- 技术摘要区块 -->
                        <div class="technical-abstract border-top pt-3">
                            <p class="abstract-paragraph">
                            采用深度学习估计单应性矩阵，缓解图像间更大偏移下特征匹配的难题，并提出一种高效且具有可扩展性的单应性矩阵网络设计。
                            </p>
                            <p class="abstract-paragraph">
                            <strong>方法：</strong>(1) 基于ShuffleNetV2压缩单元的基础网络结构和两种提升预测精度的扩展性结构，更好地平衡单应性矩阵估计精度和推断速度。
			    </p>
                       </div>
                    </div>
                </div>
                           
            </div>
        </div>
    </div>


    <div id='footer'>
        <p style="text-align:right;font-size:small;" data-translate="footer.note"></p>
    </div>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    
    <script>
    // 语言配置
    const translations = {
        en: {
	    header: {
                name: "Yilei Chen",
                title: "Collaborative Ph.D. Student",
                affiliation: "Shanghai University; University of Technology Sydney",
                email: "yileichen@shu.edu.cn"
            },
            bio: {
		    title: "Bio",
		    content: `I'm currently working toward the collaborative Ph.D. degree between Shanghai University supervised by <a href="https://scie-ie.shu.edu.cn/info/1078/1139.htm">Prof. Ping An</a> and the University of Technology Sydney supervised by <a href="https://profiles.uts.edu.au/qiang.wu">A/Prof. Qiang Wu</a>. Before that, I received my B.E. degree in 2018 from the School of Communication and Information Engineering, Shanghai University.`,
	            research_interest: "My research interests include light field/multi-view image processing.",
		    projects_title: "Projects",
	    },
	    publications_first: {
		    title: "Publications",
		    note: "* denotes equal contribution",
	    },
	    publications_other: {
		    title: "Other Authored Publications",
	    },
            footer: {
                note: "This website template is adapted from <a href='https://www.vincentsitzmann.com/'>here</a>.<br>Last updated April 2025."
            }
	    papers: {
	            vfavet: {
			    abstract1: "A generalizable neural radiance field (GNeRF) method for high-quality unstructured view synthesis.",
                    },
            },
	// 对应中文翻译
        zh: {
            header: {
                name: "陈亦雷（Yilei Chen）",
                title: "联合培养博士研究生",
                affiliation: "上海大学；悉尼科技大学",
                email: "yileichen@shu.edu.cn"
            },
	    bio: {
		    title: "个人简介",
		    content: `我目前在悉尼科技大学（University of Technology，UTS）攻读上海大学-UTS双学位联合培养博士研究生，导师为上海大学<a href="https://scie-ie.shu.edu.cn/info/1078/1139.htm">安平</a>教授与悉尼科技大学<a href="https://profiles.uts.edu.au/qiang.wu">Qiang Wu</a>副教授。2018年于上海大学通信与信息工程学院获得工学学士学位。`,
	            research_interest: "主要研究方向是光场（Light Field）/多视点图像处理，包括光场数据的稀疏性探索、基于光场深度信息的密集视点合成、压缩、显著性目标检测。",
		    projects_title: "项目经历",
	    },
            publications_first: {
		    title: "发表论文",
		    note: "* 为共同第一作者",
	    },
            footer: {
                note: "网页模板参考自<a href='https://www.vincentsitzmann.com/'>此处</a>。<br>最后更新于2025年4月。"
            }
	    papers: {
	            vfavet: {
			    abstract1: "一种用于高质量非结构化视点合成的泛化神经辐射场（Generalizable Neural Radiance Field，GNeRF）方法。",
		    },
        }
    };

    function updateContent(lang) {
        // 通用翻译处理
        document.querySelectorAll('[data-translate]').forEach(el => {
		const keys = el.dataset.translate.split('.');
		let value = translations[lang];
		keys.forEach(key => value = value?.[key]);
		if (value) {
			el.innerHTML = value; // 使用innerHTML保留超链接
		}
	});
    }

    function setLanguage(lang) {
        document.querySelectorAll('.lang-btn').forEach(btn => btn.classList.remove('active'));
        document.querySelector(`#${lang}-btn`).classList.add('active');
        localStorage.setItem('preferredLang', lang);
        updateContent(lang);
    }

    // 初始化语言
    const preferredLang = localStorage.getItem('preferredLang') || 'zh';  // 默认改为zh
    setLanguage(preferredLang);

    // 按钮点击事件
    document.getElementById('en-btn').onclick = () => setLanguage('en');
    document.getElementById('zh-btn').onclick = () => setLanguage('zh');
</script>
</body>

</html>
